# **Machine Learning on Big Data (CN7030) CRWK 21-22**
# **Group ID: 11**
1.   Student 1: Aekkaraj Kuplakatee ID: u2235955
2.   Student 2: Stevin Sam ID: u1802868
---
#####Module leader: **Dr Amin Karami** (a.karami@uel.ac.uk)


# **Initiate and Configure Spark**

---


#install Apache Spark framwork using python programming language

#install pyspark for python API to interact with spark
!pip3 install pyspark
#Install findspark to set up spark environment 
!pip3 install findspark

import os       #importing os to set environment variable
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk
  !java -version       #check java version
install_java()

#Configure local Spark using spark session
from pyspark.sql import SparkSession

#Initate Sapark Session to interact with the dataframe 
spark = SparkSession\
  .builder\
  .appName("G11_ML_Classification")\
  .config("spark.some.config.option", "some-value")\
  .getOrCreate()

#Importing libraries
from google.colab import drive
import pyspark.sql.functions as F
import numpy as np
from pyspark.sql.functions import when
from pyspark.sql.types import StringType, DoubleType
from pyspark.ml.feature import Imputer
from pyspark.sql.functions import col,isnan,count
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import UnivariateFeatureSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import IndexToString
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix


# **Load Data**

---


#Access the dataset through google drive

drive.mount('/content/drive')

#Load the dataset

DF = spark.read.csv('drive/My Drive/Machine Learning/UNSW-NB15.csv',inferSchema=True, header=None)

# **Data Preprocessing**

#Assign the header columns to dataset
new_col_names = ['Srcip', 'Sport', 'Dstip', 'Dsport','Proto', 'State', 'Dur', 'Sbytes', 'Dbytes', 'Sttl', 'Dttl', 'Sloss', 'Dloss', 'Service', 'Sload', 
            'Dload', 'Spkts', 'Dpkts', 'Swin', 'Dwin', 'Stcpb', 'Dtcpb', 'Smeansz', 'Dmeansz', 'Trans_depth', 'Res_bdy_len', 'Sjit', 'Djit', 'Stime', 
            'Ltime', 'Sintpkt', 'Dintpkt', 'Tcprtt', 'Synack', 'Ackdat', 'Is_sm_ips_ports', 'Ct_state_ttl', 'Ct_flw_http_mthd', 'Is_ftp_login', 'Ct_ftp_cmd',
            'Ct_srv_src', 'Ct_srv_dst', 'Ct_dst_ltm', 'Ct_src_ltm', 'Ct_src_dport_ltm', 'Ct_dst_sport_ltm', 'Ct_dst_src_ltm', 'Attack_cat', 'Label']

#Add the header columns to the dataframe
DF = DF.toDF(*new_col_names)

#Chechk how manny 0 values are in the dataset
DF.select([F.count(F.when(DF[c] == 0, c)).alias(c) for c in DF.columns]).show(truncate=False, vertical=True)

#Clean the dataset by changing all of the 0 values to nan values
for zCol in DF.columns: DF = DF.withColumn(zCol,when(DF[zCol] ==0,np.nan).otherwise(DF[zCol]))
#Because we will use Label column to perform binary classification. SO, we change nan values back to 0 values
DF = DF.withColumn("Label",when(DF.Label == np.nan,0).otherwise(DF.Label))
#Set all missing values in the Attack_cat column to "Noattack" 
DF = DF.withColumn("Attack_cat",when(DF.Attack_cat.isNull(), "NoAttack").otherwise (DF.Attack_cat))

DF.show(2,truncate=False,vertical=True)

# seperate the string from integer columns to be able to process the Imputer
strCols = [f.name for f in DF.schema.fields if isinstance(f.dataType, StringType)]
dblCols = [f.name for f in DF.schema.fields if isinstance(f.dataType, DoubleType)]

# fill the missing values with the imputer function
imputer=Imputer(inputCols=dblCols,outputCols=dblCols)
model=imputer.fit(DF)
DF=model.transform(DF)

DF.show(2,truncate=False,vertical=True)

#check the missing values for both nan and null 
DF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in DF.columns]).show(truncate=False,vertical=True)

#numeric indexing for the strings (indexing starts from 0)
for cols in strCols:
    indexer = StringIndexer(inputCol=cols, outputCol=cols+"_index")
#fit the indexer model and use it to transform the strings into numeric indices
DF = indexer.fit(DF).transform(DF)

#one-hot-encoding the numeric indices
for cols in strCols:
  ohe = OneHotEncoder(inputCol=cols+"_index" , outputCol=cols+"_OHEVector")
#fit the ohe model and use it to transform the numeric indices into ohe vectors
DF = ohe.fit(DF).transform(DF)

DF.show(1,truncate=False, vertical=True)

#remove the string columns before combining the dataset into the feature vector
cols=DF.columns
cols.remove("Label")
cols.remove("Dstip")
cols.remove("Proto")
cols.remove("State")
cols.remove("Service")
cols.remove("Attack_cat")
cols.remove("Srcip")

#combine all the features in one single feature vector by using Vector Assembler
assembler = VectorAssembler(inputCols=cols,outputCol="features")
DF=assembler.transform(DF)

DF.select("features").show(5,truncate=True)

#tranform the dataset to a common standad scale by using the Standard Scaler
standardscaler=StandardScaler().setInputCol("features").setOutputCol("Scaled_features")
DF=standardscaler.fit(DF).transform(DF)

DF.select("features","Scaled_features").show(5,truncate=True)


# **Task 1 - Binary Classifier**

---


#For the binary classifier we use Logistic regression to predict the result of the dataset. 

# Split the dataset into train data and test data using random split in the binary classification (trian: 80%, test: 20%)
trainBC, testBC = DF.randomSplit([0.8, 0.2], seed=12345)ยง1

#Observe the imbalance in the dataset using the Where function
DF_size=float(trainBC.select("Label").count())
numPositives=trainBC.select("Label").where('Label == 1').count()

per_ones=(float(numPositives)/float(DF_size))*100
numNegatives=float(DF_size-numPositives)

print('The number of ones are {}'.format(numPositives))
print('Percentage of ones are {}'.format(per_ones))

#Find the balance ratio in the dataset and balance it by using the class Weights module
BalancingRatio= numNegatives/DF_size
print('BalancingRatio = {}'.format(BalancingRatio))

trainBC=trainBC.withColumn("classWeights", when(trainBC.Label == 1,BalancingRatio).otherwise(1-BalancingRatio))
trainBC.select("classWeights").show(5,truncate=False)

#Use Univariate Feature Selector to reduce non-informative features to increase the perdiction result
ufs = UnivariateFeatureSelector(featuresCol="Scaled_features", outputCol="selectedFeatures",labelCol="Label", selectionMode="numTopFeatures")
ufs.setFeatureType("continuous").setLabelType("categorical")
trainBC=ufs.fit(trainBC).transform(trainBC)
testBC=ufs.fit(testBC).transform(testBC)

testBC.select("selectedFeatures").show(5,truncate=True)

#Use the LogisticRegression classification to predict the dependency of the data
lr = LogisticRegression(labelCol="Label", featuresCol="selectedFeatures",weightCol="classWeights",maxIter=10)
model=lr.fit(trainBC)
predict_train = model.transform(trainBC)
predicted = model.transform(testBC)

predicted.select("Label", "prediction").show(5,truncate=False)

#Use the Binary Classification Evaluator to evaluate the area under the ROC curve
IRevaluator=BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol="Label")
IRaccuracy = IRevaluator.evaluate(predicted, {IRevaluator.metricName: "areaUnderROC"})

predicted.select("Label","prediction","probability").show(5,truncate=False)


# **Task 2 - Multi Classifier**

---


#For multi classifier we use two classifications, which are Random Forset and Naive Bayes Classifications to predict the result of the dataset

#Split the dataset into train data and test data using random split  in the multi classification (trian: 80%, test: 20%)
trainMC, testMC = DF.randomSplit([0.8, 0.2], seed=12345)

#Reduce non-informative features to increase the perdiction result
ufs = UnivariateFeatureSelector(featuresCol="Scaled_features", outputCol="selectedFeatures",labelCol="Attack_cat_index", selectionMode="numTopFeatures")
ufs.setFeatureType("continuous").setLabelType("categorical")
trainMC=ufs.fit(trainMC).transform(trainMC)
testMC=ufs.fit(testMC).transform(testMC)

testMC.select("selectedFeatures").show(5,truncate=True)

#Use the Random Forest Classification to predict the result of the data
rf = RandomForestClassifier(labelCol="Attack_cat_index", featuresCol="selectedFeatures", numTrees=10)
rfmodel=rf.fit(trainMC)
predict_trainMC = rfmodel.transform(trainMC)
rfpredicted = rfmodel.transform(testMC)

rfpredicted.select("Attack_cat", "prediction").show(5,truncate=False)

#Use the Naive Bayes classification to predict the result of the data 
nb = NaiveBayes(featuresCol='selectedFeatures', labelCol="Attack_cat_index")
nbmodel=nb.fit(trainMC)
predict_trainnb = nbmodel.transform(trainMC)
nbpredicted = nbmodel.transform(testMC)

nbpredicted.select("Attack_cat", "prediction").show(5,truncate=False)

#Convert indexed labels back to original labels.
labelConverter = IndexToString(inputCol="Attack_cat_index", outputCol="predictedLabel")
lbpredicted = labelConverter.transform(testMC)

lbpredicted.select("Attack_cat", "predictedLabel").show(5,truncate=False)

#Use the Multiclass classification Evaluator to evaluate the prediction accuracy 
evaluator = MulticlassClassificationEvaluator(labelCol="Attack_cat_index", predictionCol="prediction", metricName="accuracy")
rfAccuracy = evaluator.evaluate(rfpredicted)
nbAccuracy = evaluator.evaluate(nbpredicted)

rfpredicted.select("Attack_cat","prediction","probability").show(5,truncate=True)
nbpredicted.select("Attack_cat","prediction","probability").show(5,truncate=True)

# **Task 3 - Performance Measurements**

## **Measurements for Binary Classifer** ##

#check the percentage of the area under the ROC curve
print("Area under ROC Curve: {:.4f}".format(IRaccuracy))
print(model.summary)

#visualise the accuracy of the prediction
pr = model.summary.pr.toPandas()
plt.plot(pr['recall'],pr['precision'])
plt.ylabel('Precision')
plt.xlabel('Recall')

plt.show()

#show the percentage of acurracy of the perdiction  
print("Model Accuracy",model.summary.accuracy)
#show the false positive rate
print("FP rate",model.summary.falsePositiveRateByLabel)
#show the true positive rate
print("TR rate",model.summary.truePositiveRateByLabel)
#show the total count of actual positive and actual negative
print("Total Actual Positive i.e. Attack Record",predicted.select("Label").where('Label == 1.0').count())
print("Total Actual Negative,i.e. Normal",predicted.select("Label").where('Label == 0.0').count())

#defining the fucntion to plt a confrusion matrix
#visualise the total count of the positive and negative results by using the confusion matrix method
def plot_cnf_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):
  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("Normalized confusion matrix")
  else:
    print('Confusion matrix, without normalization') 
    
    print(cm) 
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes) 
    
    fmt = '.2f' if normalize else 'd' 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): 
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black") 
      plt.tight_layout()
      plt.ylabel('True label')
      plt.xlabel('Predicted label')


y_true = testBC.select("Label")
y_true = y_true.toPandas()
y_pred = predicted.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred)
cnf_matrix

plt.figure()
plot_cnf_matrix(cnf_matrix, classes=[1,0],
title='Attack Record, Normal')
plt.show()

plt.figure()
plot_cnf_matrix(cnf_matrix, classes=[0,1], normalize=True,
title='Normalized confusion matrix')

plt.show()

FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  
FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)
TP = np.diag(cnf_matrix)
TN = cnf_matrix.sum() - (FP + FN + TP)

# Sensitivity, hit rate, recall, or true positive rate
TPR = TP/(TP+FN)
print(TPR)

# Specificity or true negative rate
TNR = TN/(TN+FP) 
print(TNR)

# Precision or positive predictive value
PPV = TP/(TP+FP)
print(PPV)

# Negative predictive value
NPV = TN/(TN+FN)
print(NPV)

# Fall out or false positive rate
FPR = FP/(FP+TN)
print(FPR)

# False negative rate
FNR = FN/(TP+FN)
print(FNR)

# False discovery rate
FDR = FP/(TP+FP)
print(FDR)

# Overall accuracy
ACC = (TP+TN)/(TP+FP+FN+TN)
print(ACC)

## **Measurements for Multi Classfier** ##

#Show the accuracy results
print("Random Forest:")
print("Test Error = %g" % (1 - rfAccuracy))
print("Test accuracy = ", rfAccuracy)

#Show the accuracy results
print("Naive Bayes:")
print("Test Error = %g" % (1 - nbAccuracy))
print("Test accuracy = ", nbAccuracy)

#Visualise the comparision of the Random Forest and Naive Bayes model
cmpAcc=[rfAccuracy, nbAccuracy]
fig = go.Figure([go.Bar(x=["Random Forest","Naive Bayes"], y=cmpAcc)])
fig.update_layout(
    title="Comparing the Random Forest and Naive Bayes model accuracies",
    autosize=False,
    width=600,
    height=450,
    )
fig.show()

#defining the fucntion to plt a confrusion matrix (use Random Forest Classification as the main result)
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Reds):

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#processing the classes for the confusion matrix 
class_temp = rfpredicted.select("Attack_cat").groupBy("Attack_cat").count().sort('count', ascending=False).toPandas()
class_temp = class_temp["Attack_cat"].values.tolist()
class_names = map(str, class_temp)

class_names

#visualse the result of the predictions by using the confusion matrix method
y_true = rfpredicted.select("Attack_cat")
y_true = y_true.toPandas()

y_pred = lbpredicted.select("predictedLabel")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred, labels = class_temp)

plt.figure(figsize=(15,10))
plot_confusion_matrix(cnf_matrix, classes=class_temp, title='Confusion matrix, without normalization')
plt.show()

# **Convert ipynb to HTML for Turnitin submission**

---



!pip3 install nbconvert 
