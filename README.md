# Machine Learning Applications for Big Data Analytics

This repository contains a machine learning project that focuses on analyzing the UNSW-NB15 dataset using Apache Spark for big data analytics. The UNSW-NB15 dataset consists of raw network packets generated by the IXIA PerfectStorm tool in the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS). The dataset combines real modern normal activities with synthetic contemporary attack behaviors.

## Spark Machine Learning on a real case study
### Overview

The project involves several key steps:

1. **Initiating and Configuring Spark:**
   - The Apache Spark framework is installed using the Python programming language.
   - PySpark, findspark, and other necessary libraries are installed to set up the Spark environment.
   - Local Spark is configured using Spark session for data interaction.
   
2. **Data Loading and Preprocessing:**
   - The dataset is accessed through Google Drive and loaded into the Spark environment.
   - Header columns are assigned to the dataset.
   - Data preprocessing steps include handling missing values, converting 0 values to NaN, and filling missing values using imputation techniques.
   - String and integer columns are separated for processing with the Imputer.
   - One-hot encoding is applied to numeric indices, and feature vectors are assembled using Vector Assembler.
   - Data is transformed to a common standard scale using Standard Scaler.
   
3. **Binary Classification:**
   - Logistic regression is used as a binary classifier to predict the dataset's results.
   - The dataset is split into training and testing data (80% train, 20% test).
   - Imbalance in the dataset is observed and balanced using class weights.
   - Non-informative features are reduced using Univariate Feature Selector.
   - Logistic regression classification is applied, and evaluation is done using the Binary Classification Evaluator, focusing on the area under the ROC curve.

4. **Multi-Class Classification:**
   - Two classifiers, Random Forest and Naive Bayes, are used for multi-class classification.
   - The dataset is split into training and testing data (80% train, 20% test).
   - Non-informative features are reduced using Univariate Feature Selector.
   - Random Forest and Naive Bayes classifications are applied, and results are evaluated using the Multiclass Classification Evaluator.

5. **Performance Measurements:**
   - Performance metrics such as area under the ROC curve and accuracy are calculated for the binary classifier.
   - Confusion matrices are visualized to understand the model's performance.
   - Test error and accuracy are reported for both Random Forest and Naive Bayes classifiers in multi-class classification.

## Libraries Used
- `google.colab` (for Google Drive access)
- `pyspark.sql.functions`
- `numpy`
- `pyspark.ml.feature.Imputer`
- `pyspark.ml.feature.StringIndexer`
- `pyspark.ml.feature.OneHotEncoder`
- `pyspark.ml.feature.VectorAssembler`
- `pyspark.ml.feature.StandardScaler`
- `pyspark.ml.feature.UnivariateFeatureSelector`
- `pyspark.ml.classification.LogisticRegression`
- `pyspark.ml.classification.RandomForestClassifier`
- `pyspark.ml.classification.NaiveBayes`
- `pyspark.ml.feature.IndexToString`
- `pyspark.ml.Pipeline`
- `pyspark.ml.evaluation.BinaryClassificationEvaluator`
- `pyspark.ml.evaluation.MulticlassClassificationEvaluator`
- `plotly.graph_objects`
- `matplotlib.pyplot`
- `sklearn.metrics.confusion_matrix`

## Spark Streaming for a Streaming-based Application

### Overview
1. **Initiate and Configure Spark Streaming**
- The Spark Streaming application is initiated and configured to read streaming data from a local server using the nc command on port 7777.
- PySpark and findspark libraries are utilized for initializing and configuring the Spark engine.
- The application defines functions for indexing the input data and processing each batch of input data.
2. **Input Data Processing**
- The input data is processed in batches, where each batch undergoes a series of operations.
- The input dataframe is indexed and split into Odd and Even rows based on the row number.
- Non-digit characters in the 'Even' dataframe are removed, and Odd and Even dataframes are combined.
- Input sentences are split into individual words, and their lengths are calculated.
- Inputs are classified based on their length and presence of digits:
  - Inputs with a length less than 5 characters are classified as "null."
  - Inputs with a length greater than or equal to 5 characters are classified as "Word Length >= 5."
  - Inputs containing digits are preserved as is.
- Valid inputs are displayed, and the application performs word count aggregation.
- The final processed data, including word counts, is displayed in the console output.
3. **Main Program**
- The main function initiates a local Spark session with two working threads.
- The streaming data is read from the socket connection and processed in real-time using the defined functions.
- The application runs indefinitely, continuously processing streaming data until manually terminated.
